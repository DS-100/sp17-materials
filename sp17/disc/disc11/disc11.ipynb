{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Discussion 11: Logistic Regression and Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches, cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.display import display, Latex, Markdown\n",
    "from ipywidgets import interact, interactive, fixed\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to better understand gradient descent, let's implement it to solve a familiar problem - least-squares linear regression. While we are able to find the solution to ordinary least-squares linear regression analytically (recall its value as $\\theta = (X^TX)^{âˆ’1}X^TY$), we can also find it using gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1:\n",
    "First, let's consider the gradient function for ordinary least squares regression. Recall the OLS loss function as\n",
    "\n",
    "$$Loss(\\theta) = \\frac{1}{n} \\sum_{i=1}^n \\left(y_i - f_\\theta(x_i)\\right)^2$$\n",
    "\n",
    "And the function $f_\\theta(x_i)$, for input data with $p$ dimensions, as\n",
    "\n",
    "$$f_\\theta(x_i) = \\sum_{j=1}^p \\theta_j x_{i,j} $$\n",
    "\n",
    "Given these functions, what is the gradient function for OLS regression? First, state it in terms of a single component of $\\theta$, $\\theta_j$, using a sum over each data point $i$ in $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q1_answer = r\"\"\"\n",
    "\n",
    "Put your answer here, replacing this text.\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\theta_j} Loss(\\theta) = \\frac{1}{n} \\sum_{i=1}^n \\dots$$\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "display(Markdown(q1_answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q1_answer = r\"\"\"\n",
    "\n",
    "*Write your answer here, replacing this text.*",
    "\n",
    "$$\\frac{\\partial}{\\partial \\theta_j} Loss(\\theta) = \\frac{2}{n} \\sum_{i=1}^n -x_{i,j} \\left(y_i - f_\\theta(x_i)\\right)$$\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "display(Markdown(q1_answer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2:\n",
    "\n",
    "Now, try to write that formula in terms of the matricies $X$, $y$, and $\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q2_answer = r\"\"\"\n",
    "\n",
    "Put your answer here, replacing this text.\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\theta} Loss(X)  = \\dots$$\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "display(Markdown(q2_answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q2_answer = r\"\"\"\n",
    "\n",
    "*Write your answer here, replacing this text.*",
    "\n",
    "$$\\frac{\\partial}{\\partial \\theta} Loss(X)  = -\\frac{2}{n} X^T (y - X^T \\theta)$$\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "display(Markdown(q2_answer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3:\n",
    "Using this gradient function, complete the python function below which calculates the gradient for inputs $X$, $y$, and $\\theta$. You should get a gradient of $[7, 48]$ on the simple data below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def linear_regression_grad(X, y, theta):\n",
    "    grad = ...\n",
    "    return grad\n",
    "\n",
    "theta = [1, 4]\n",
    "simple_X = np.vstack([np.ones(10), np.arange(10)]).T \n",
    "simple_y = np.arange(10) * 3 + 2\n",
    "linear_regression_grad(simple_X, simple_y, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4:\n",
    "\n",
    "Before we perform gradient descent, let's visualize the surface we're attempting to descend over. Run the next few cells to plot the loss surface as a function of $\\theta_0$ and $\\theta_1$, for some toy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_surface_3d(X, Y, Z, angle):\n",
    "    highest_Z = max(Z.reshape(-1,1))\n",
    "    lowest_Z = min(Z.reshape(-1,1))\n",
    "    fig = plt.figure()\n",
    "    ax = fig.gca(projection='3d')\n",
    "    surf = ax.plot_surface(X, Y, Z, \n",
    "                           cmap=cm.coolwarm, \n",
    "                           linewidth=0, \n",
    "                           antialiased=False, \n",
    "                           rstride=5, cstride=5)\n",
    "    ax.zaxis.set_major_locator(LinearLocator(5))\n",
    "    ax.zaxis.set_major_formatter(FormatStrFormatter('%.1f'))\n",
    "    ax.view_init(45, angle)\n",
    "    fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "    plt.title(\"Regression Loss Function\")\n",
    "    plt.xlabel(\"Theta_0\")\n",
    "    plt.ylabel(\"Theta_1\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create some toy data in two dimensions to perform our regressions on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(100)\n",
    "X_1 = np.arange(50)/5 + 5\n",
    "X = np.vstack([np.ones(50), X_1]).T \n",
    "y = (X_1 * 2 + 3) + np.random.normal(0, 2.5, size=50)\n",
    "plt.plot(X_1, y, \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And plot our loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "angle_slider = widgets.FloatSlider(min=0, max=360, step=15, value=45)\n",
    "\n",
    "def plot_regression_loss(angle):\n",
    "\n",
    "    t0_vals = np.linspace(-10,10,100)\n",
    "    t1_vals = np.linspace(-2,5,100)\n",
    "    theta_0,theta_1 = np.meshgrid(t0_vals, t1_vals)\n",
    "    thetas = np.vstack((theta_0.flatten(), theta_1.flatten()))\n",
    "    loss_vals = 2/X.shape[0] * sum(((y - (X @ thetas).T)**2).T)\n",
    "    loss_vals = loss_vals.reshape(100, -100)\n",
    "    plot_surface_3d(theta_0, theta_1, loss_vals, angle)\n",
    "    \n",
    "interact(plot_regression_loss, angle=angle_slider);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider: \n",
    "- What do you notice about the loss surface for this simple regression example? \n",
    "- Where are the optimal values $(\\theta_0, \\theta_1)$? \n",
    "- Do you think that the shape of this surface will make gradient descent a viable solution to find these optimal values? \n",
    "- What other loss surface shapes could you imagine?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5:\n",
    "Now, let's implement a general function to perform batch gradient descent. Given data X and y, initial weights $\\theta_0$, a learning rate $\\rho$, and a function `gradient_function` that has the same function signature as `linear_regression_grad`, implement a general gradient descent algorithm for finding optimal $\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, theta0, gradient_function, learning_rate = 0.001, max_iter=1000000,  epsilon=0.001):\n",
    "    \n",
    "    theta_hat = theta0 # Initial guess\n",
    "    for t in range(1, max_iter):\n",
    "        \n",
    "        grad = gradient_function(X, y, theta_hat)\n",
    "        \n",
    "        # Now for the update step\n",
    "        theta_hat = ...\n",
    "        \n",
    "        # When our gradient is small enough, we have converged\n",
    "        if np.linalg.norm(grad) < epsilon:\n",
    "            print(\"converged after {} steps\".format(t))\n",
    "            return theta_hat\n",
    "    \n",
    "    # If we hit max_iter iterations\n",
    "    print(\"Warning - Failed to converge\")\n",
    "    return theta_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "theta_0 = [10, -1]\n",
    "gradient_descent(X, y, theta_0, linear_regression_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's visualize how our regression estimates change as we perform gradient descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "theta_0s = []\n",
    "theta_1s = []\n",
    "plot_idx = [1, 5, 20, 100, 500, 2000, 10000]\n",
    "\n",
    "def plot_gradient_wrapper(X, y, theta):\n",
    "    grad = linear_regression_grad(X, y, theta)\n",
    "    theta_0s.append(theta[0])\n",
    "    theta_1s.append(theta[1])\n",
    "    t = len(theta_0s)\n",
    "    if t in plot_idx:\n",
    "        plt.subplot(121)\n",
    "        plt.xlim([4, 12])\n",
    "        plt.ylim([-2, 3])\n",
    "        plt.plot(theta_0s, theta_1s)\n",
    "        plt.plot(theta[0], theta[1], \".\", color=\"b\")\n",
    "        plt.title('theta(s) over time, t={}'.format(t))\n",
    "        plt.subplot(122)\n",
    "        plt.xlim([0, 20])\n",
    "        plt.ylim([-10, 40])\n",
    "        plt.plot(np.arange(50)/2.5, y, \".\")\n",
    "        plt.plot(np.arange(50)/2.5, X @ theta)\n",
    "        plt.title('Regression line vs. data, t={}'.format(t))\n",
    "        plt.show()\n",
    "    return grad\n",
    "\n",
    "gradient_descent(X, y, theta_0, plot_gradient_wrapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 6:\n",
    "\n",
    "In Prof. Gonzalez's lecture, instead of using a constant learning rate, he used a learning rate that decreased over time, according to a function:\n",
    "$$\\rho(t) = \\frac{r}{t}$$\n",
    "Where $r$ represents some initial learning rate. This has the feature of decreasing the learning rate as we get closer to the optimal solution.\n",
    "- Why might this be useful, compared to a constant learning rate? \n",
    "- What problems might be caused by using too high of a learning rate? \n",
    "- What about too low?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extending to Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 7\n",
    "\n",
    "As discussed in lecture, while ordinary least squares has a simple analytical solution, logistic regression must be fitted using gradient descent. Using the tools we've constructed, we can do just that. First, create a new function, `logistic_regression_grad`, which functions similarly to its counterpart `linear_regression_grad`. In the case of logistic regression, this should be the gradient of the logistic regression log-likelihood function - you may wish to refer to the lecture slides to find this gradient equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define the sigmoid function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    return 1/(1 + np.e**-t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then complete the gradient function. You should get a gradient of about $[0.65, 0.61]$ for the given values $\\theta$ on this example dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def logistic_regression_grad(X, y, theta):\n",
    "    grad = ...\n",
    "    return grad\n",
    "\n",
    "theta = [0, 1]\n",
    "simple_X_1 = np.hstack([np.arange(10)/10, np.arange(10)/10 + 0.75])\n",
    "simple_X = np.vstack([np.ones(20), simple_X_1]).T\n",
    "simple_y = np.hstack([np.zeros(10), np.ones(10)])\n",
    "linear_regression_grad(simple_X, simple_y, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how we can use our gradient descent tools to fit a regression on some real data! First, let's load the breast cancer dataset from lecture, and plot breast mass radius versus category - malignant or benign. As in lecture, we jitter the response variable to avoid overplotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "data_dict = sklearn.datasets.load_breast_cancer()\n",
    "data = pd.DataFrame(data_dict['data'], columns=data_dict['feature_names'])\n",
    "data['malignant'] = (data_dict['target'] == 0)\n",
    "data['malignant'] = data['malignant'] + 0.1*np.random.rand(len(data['malignant'])) - 0.05\n",
    "\n",
    "X_log_1 = data['mean radius']\n",
    "X_log = np.vstack([np.ones(len(X_log_1)), X_log_1.values]).T\n",
    "y_log = data['malignant'].values\n",
    "plt.plot(X_log_1, y_log, \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 8:\n",
    "\n",
    "Now, using our earlier defined `gradient_descent` function, find optimal parameters $(\\theta_0, \\theta_1)$ to fit the breast cancer data. You will have to tune the learning rate beyond the default of the function, and think of what a good initial guess for $\\theta$ would be, in both dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "theta_log = ...\n",
    "theta_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With optimal $\\theta$ chosen, we can now plot our logistic curve and our decision boundary, and look at how our model categorizes our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_lowX = X_log_1[sigmoid(X_log @ theta_log) < 0.5]\n",
    "y_lowy = y_log[sigmoid(X_log @ theta_log) < 0.5]\n",
    "y_highX = X_log_1[sigmoid(X_log @ theta_log) > 0.5]\n",
    "y_highy = y_log[sigmoid(X_log @ theta_log) > 0.5]\n",
    "\n",
    "sigrange = np.arange(5, 30, 0.05)\n",
    "sigrange_X = np.vstack([np.ones(500), sigrange]).T\n",
    "d_boundary = -theta_log[0]/theta_log[1]\n",
    "\n",
    "plt.plot(sigrange, sigmoid(sigrange_X @ theta_log), \".\", color=\"g\")\n",
    "plt.hlines(0.5, 5, 30, \"g\")\n",
    "plt.vlines(d_boundary, -0.2, 1.2, \"g\")\n",
    "plt.plot(y_lowX, y_lowy, \".\", color=\"b\")\n",
    "plt.plot(y_highX, y_highy, \".\", color=\"r\")\n",
    "plt.title(\"Classification (blue=benign, red=malignant), assuming a P=0.5 decision boundary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, we can calculate our classification accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_errors = sum(y_lowy > 0.5) + sum(y_highy < 0.5)\n",
    "accuracy = round((len(y_log)-n_errors)/len(y_log) * 1000)/10\n",
    "print(\"Classification Accuracy - {}%\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}


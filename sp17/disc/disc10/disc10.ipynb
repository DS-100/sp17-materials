{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# The Bias-Variance Trade-Off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 12, 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In this discussion, we're going to check numerically some of the results from yesterday's lecture. Speicifically, we're going to see how bias and variance change as we modify model complexity.\n",
    "\n",
    "We'll start by defining some toy data and using it to build our models. In this case, we'll use the sum of some sine waves as our underlying function $h(x)$, plus some gaussian noise. Run the cell below to see an example set of this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(100)\n",
    "x = np.array([i*np.pi/180 for i in range(0,360,18)])\n",
    "h = lambda x: np.sin(x) + np.cos(x) + np.sin(2*x) + np.cos(2*x)\n",
    "y = h(x) + np.random.normal(0,0.7,len(x))\n",
    "plt.plot(x, y, '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We're going to compare a number of models we might use to fit this data, using polynomial regression. Specifically, we're going to compare models with polynomial terms varying from 1 to 20. The cell below implements a polynomial regression of degree $p$, given data $x$ and $y$.\n",
    "\n",
    "Recall that a polynomial regression of degree $p$ will find weights $\\theta$ to fit the following equation:\n",
    "\n",
    "$$f_\\theta(x) = \\theta_0 + \\theta_1 x + \\theta_2 x^2 + ... + \\theta_{p-1} x^{p-1} + \\theta_p x^p$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "def poly_regression(x, y, p):\n",
    "    \n",
    "    reg = LinearRegression(normalize=True)\n",
    "    poly = PolynomialFeatures(degree=p)\n",
    "    poly_basis = poly.fit_transform(x.reshape(-1,1))\n",
    "    reg.fit(poly_basis, y)\n",
    "    f_theta = reg.predict(poly_basis)\n",
    "    \n",
    "    return f_theta\n",
    "\n",
    "def smooth_curve(x, y, p):\n",
    "    \n",
    "    # computes predictions for all more values, to get a smoother curve\n",
    "    # you will not need to use this function\n",
    "    \n",
    "    reg = LinearRegression(normalize=True)\n",
    "    poly = PolynomialFeatures(degree=p)\n",
    "    poly_basis = poly.fit_transform(x.reshape(-1,1))\n",
    "    reg.fit(poly_basis, y)\n",
    "    \n",
    "    x_extended = np.array([i*np.pi/180 for i in range(0,360,6)])\n",
    "    poly_extended = PolynomialFeatures(degree=p)\n",
    "    poly_basis_extended = poly_extended.fit_transform(x_extended.reshape(-1,1))\n",
    "    f_theta = reg.predict(poly_basis_extended)\n",
    "    \n",
    "    return f_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Question 1:\n",
    "We plot some of these prediction functions below, with the prediction curve $f_\\theta(x)$ for each in green. What causes the change in shape between various prediction curves? Which models seem to be overfitting? Which are underfitting? Which models have lower bias? Which have lower variance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "curve = np.array([i*np.pi/180 for i in range(0,360,6)])\n",
    "plots = [(1,231),(3,232),(5,233),(10,234),(15,235),(30,236)]\n",
    "\n",
    "for p, subplot in plots:\n",
    "    plt.subplot(subplot)\n",
    "    plt.plot(x, y, \".\")\n",
    "    plt.plot(curve, smooth_curve(x, y, p))\n",
    "    plt.title('Plot for p={}'.format(p))\n",
    "    plt.ylim((-3,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now let's consider the three componets that make up our squared error:\n",
    "\n",
    "$$\\mathbf{E}\\left[\\left(y - f_\\theta (x)\\right)^2\\right] = \n",
    "\\mathbf{E}\\left[\\left(y - h (x)\\right)^2\\right] + \n",
    "\\mathbf{E}\\left[\\left(h(x) - \\mathbf{E}_\\theta[f_\\theta (x)]\\right)^2\\right] +\n",
    "\\mathbf{E}\\left[\\left(\\mathbf{E}_\\theta[f_\\theta (x)] - f_\\theta (x)\\right)^2\\right]$$\n",
    "\n",
    "Recall from lecture that these three components represent:\n",
    "\n",
    "Noise: $\\mathbf{E}\\left[\\left(y - h (x)\\right)^2\\right]$\n",
    "\n",
    "Bias$^2$: $\\mathbf{E}\\left[\\left(h(x) - \\mathbf{E}_\\theta[f_\\theta (x)]\\right)^2\\right]$\n",
    "\n",
    "Variance: $\\mathbf{E}\\left[\\left(\\mathbf{E}_\\theta[f_\\theta (x)] - f_\\theta (x)\\right)^2\\right]$\n",
    "\n",
    "We want to calculate the necessary terms numerically.\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Note that the noise term does not depend in any way on our model. Since $y = h(x) + \\epsilon$, we see that $\\mathbf{E}\\left[\\left(y - h (x)\\right)^2\\right] = \\mathbf{E}[\\epsilon^2]$. And since we are using $\\epsilon \\sim \\mathcal{N}(0, 0.7)$, this is simply $0.49$, regardless of our model.\n",
    "\n",
    "*You don't need to know how to derive this, but if you're interested, note that $\\mathbf{E}[X^2] = 1$ when $X \\sim \\mathcal{N}(0, 1)$. From this we see that, if $\\epsilon = 0.7X$, then*\n",
    "$$\\mathbf{E}[\\epsilon^2] = \\mathbf{E}[(0.7X)^2] = \\mathbf{E}[0.49 X^2] = 0.49\\mathbf{E}[X^2] = 0.49$$\n",
    "<br>\n",
    "<br>\n",
    "Now, let's calculate the other two terms - Bias$^2$ and Variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Question 2\n",
    "First, let's consider the term $\\mathbf{E}_\\theta[f_\\theta (x)]$. What does this term represent? Is it a constant or a variable? If it's a variable, is its value dependent on the specific data $x$ that we have? What about on our model $\\theta$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Question 3\n",
    "We can calculate $\\mathbf{E}_\\theta[f_\\theta (x)]$ by simulating a series of datasets and computing a model on each dataset, then getting the expected value of each point $x_i$ across all models. Every dataset should be created in the same manner - by generating random values $\\epsilon$ and calculating $y = h(x) + \\epsilon$, where $\\epsilon \\sim \\mathcal{N}(0, 0.7)$.\n",
    "\n",
    "Fill in the function below to calculate $\\mathbf{E}_\\theta[f_\\theta (x)]$ in this manner. The function `E_f_theta_x` takes in a single argument $p$, which represents the degree of the polynomial basis that will be used to create the function $f_\\theta(x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def E_f_theta_x(p, n_sims=1000, sigma=0.7):\n",
    "    f_thetas = np.zeros(20)\n",
    "    for sim in range(n_sims):\n",
    "        y = ...\n",
    "        f_thetas = f_thetas + poly_regression(x, y, p)\n",
    "    E_f_thetas = ...\n",
    "    return E_f_thetas\n",
    "E_f_theta_x(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Question 4\n",
    "\n",
    "Now let's compute the bias term for a variety of different models. Fill in the `bias_term` function below to calculate the squared bias of a polynomial regression of degree $p$ when fitting the function $y = f_\\theta(x) + \\epsilon$ for the provided values $x$. You should find that `bias_term(3)` $\\approx 0.547$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def bias_term(p): # really squared bias\n",
    "    bias = ...\n",
    "    return bias\n",
    "bias_term(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Question 5\n",
    "\n",
    "Now let's do the same for the variance. Fill in the `variance_term` function below to calculate the bias of a polynomial regression of degree $p$ when fitting the function $y = f_\\theta(x) + \\epsilon$ for the provided values $x$. You will need to calculate $y$ just as in question 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def variance_term(p):\n",
    "    y = ...\n",
    "    f_theta_x = poly_regression(x, y, p)\n",
    "    variance = ...\n",
    "    return variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Since your variance term will depend heavily on the specifc $\\theta$-values of each individual regression (which are in turn dependent on the specific values of $\\epsilon$ for each simulated dataset), we will average together the variance terms of a number of regressions. The function `avg_var_term` is provided below. You should find that `avg_var_term(10)` $\\approx 0.27$, though this will be less precise than for the bias term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def avg_var_term(p, n_sims=100):\n",
    "    variance = 0\n",
    "    E_f_theta = E_f_theta_x(p, sigma=0.7)\n",
    "    for sim in range(n_sims):\n",
    "        y = h(x) + np.random.normal(0, 0.7, len(x))\n",
    "        f_theta_x = poly_regression(x, y, p)\n",
    "        variance += np.mean((E_f_theta - f_theta_x)**2)\n",
    "    return variance/n_sims\n",
    "\n",
    "avg_var_term(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Question 6\n",
    "\n",
    "Now we can visualize how our bias and variance terms change as $p$ increases. \n",
    "\n",
    "**BEFORE RUNNING THE NEXT CELL:** What do you expect to happen as $p$ increases? How will the model's bias change? How will its variance change?\n",
    "\n",
    "After considering this, run the plotting cell below and see how the results match up with your expectations. It may take a minute or two for the cell to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "varis = np.array([avg_var_term(i) for i in range(1, 20)])\n",
    "bias = np.array([bias_term(i) for i in range(1, 20)])\n",
    "plt.plot(np.arange(1, 20), varis, color='b')\n",
    "plt.plot(np.arange(1, 20), bias, color='r')\n",
    "plt.title('Bias^2 and Variance vs. p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Question 7\n",
    "\n",
    "To make sure we've done our calculations correctly, let's go back to our original formula:\n",
    "\n",
    "$$\\mathbf{E}\\left[\\left(y - f_\\theta (x)\\right)^2\\right] = \n",
    "\\mathbf{E}\\left[\\left(y - h (x)\\right)^2\\right] + \n",
    "\\mathbf{E}\\left[\\left(h(x) - \\mathbf{E}_\\theta[f_\\theta (x)]\\right)^2\\right] +\n",
    "\\mathbf{E}\\left[\\left(\\mathbf{E}_\\theta[f_\\theta (x)] - f_\\theta (x)\\right)^2\\right]$$\n",
    "\n",
    "We can plot our MSE for models with various $p$ against the sums of our individual components for noise, bias$^2$, and variance. Fill in the `test_error` function below, which calculates the test MSE for a regression with degree $p$ on some simulated data. Then, run the cells below plot test error versus the sum of your computed bias$^2$ and variance, and noise. You should find that the two curves are nearly identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def test_error(p):\n",
    "    y_train = h(x) + np.random.normal(0,0.7,len(x))\n",
    "    y_test = h(x) + np.random.normal(0,0.7,len(x))\n",
    "    pred_values = poly_regression(x, y_train, p)\n",
    "    mse = ...\n",
    "    return mse\n",
    "\n",
    "def avg_test_error(p, n_sims=100):\n",
    "    return np.mean([test_error(p) for sim in range(n_sims)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "noise = 0.49\n",
    "\n",
    "plt.plot(range(1, 20), [avg_test_error(i) for i in range(1, 20)], color='b')\n",
    "plt.plot(range(1, 20), noise + bias + varis, color='g')\n",
    "plt.title(\"Test error and calculated (noise + bias^2 + variance) vs. p\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Question 8\n",
    "\n",
    "What seems to be the optimal degree $p$ for your regression? What is the average MSE across different y-sets when using that value $p$ for your model?\n",
    "\n",
    "Do you think that *any* model could achieve an average MSE much below 0.49? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Question 9\n",
    "\n",
    "Finally, let's take a look at how this test error compares to our training error. Fill in the `train_error` function below, and then run the plotting cell to see how your training error compares to your test error. Does this make sense, given what you know about the relationship between training and test error?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def train_error(p):\n",
    "    y_train = h(x) + np.random.normal(0,0.7,len(x))\n",
    "    pred_values = poly_regression(x, y_train, p)\n",
    "    mse = ...\n",
    "    return mse\n",
    "\n",
    "def avg_train_error(p, n_sims=100):\n",
    "    return np.mean([train_error(p) for sim in range(n_sims)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.plot(range(1, 20), [avg_test_error(i) for i in range(1, 20)], color='b')\n",
    "plt.plot(range(1, 20), [avg_train_error(i) for i in range(1, 20)], color='g')\n",
    "plt.title(\"Training vs. Test Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}


{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Gaussian Mixture Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# An Initial Example - 2D Data\n",
    "\n",
    "Top berkeley students are all trying to get internships at the hottest new Berkeley startups, such as UBear, DropBear, and BearBnB. We suspect that, given the different skills learned at these three companies, students who've worked at each of them will get different grades than students who worked at the others.\n",
    "\n",
    "We decide to cluster Berkeley startup interns based on their scores in CS 61A and Data 8, and attempt to derive three clusters that might map to interns at the three startups. Run the cell below to see our initial data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "np.random.seed(1337)\n",
    "vals = make_classification(n_features=2, \n",
    "                           n_informative=2, \n",
    "                           n_redundant=0, \n",
    "                           n_classes=3, \n",
    "                           n_clusters_per_class=1)\n",
    "X = vals[0] * 4 + [85, 85]\n",
    "y = vals[1]\n",
    "colors = [{0:\"red\",1:\"blue\",2:\"green\"}[l] for l in y]\n",
    "plt.scatter(X.T[0], X.T[1])\n",
    "plt.title(\"Scatter plot of CS 61A grade vs. Data 8 grade\")\n",
    "plt.xlabel(\"CS 61A Grade\")\n",
    "plt.ylabel(\"Data 8 Grade\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like there are indeed 3 clusters, that may correspond to interns from these 3 companies. Let's try running the EM algorithm to estimate gaussian parameters for these clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Part 1 - rolling our own"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "First, let's consider the simplest possible case - two gaussian classes in one dimension. We consider grades in Data 8 of students who interned at UBark and BearBnB, and try to fit gaussian parameters for each class. First, let's plot the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_simple = X[y != 2].T[1]\n",
    "y_simple = y[y != 2]\n",
    "pd.DataFrame({\"Data 8 Score\": X_simple}).plot(kind=\"density\")\n",
    "plt.title(\"Histogram of student scores in Data 8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Once again, we can see that our data is clearly bimodal, supporting our idea that it's drawn from a mixture of two gaussians. Let's try to find the parameters of the distributions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "Recall from [lecture 26](https://drive.google.com/file/d/0Bze55lezLJhIMWhYOGNNLTJKM0k/view) that we treat our data as a mixture of two gaussians,\n",
    "\n",
    "$$\\pi\\mathcal{N}(\\mu_1, \\sigma_1^2) + (1-\\pi)\\mathcal{N}(\\mu_2, \\sigma_2^2)$$\n",
    "\n",
    "That is, each data point is drawn from distribution $\\phi_1 \\sim \\mathcal{N}(\\mu_1, \\sigma_1^2)$ with probability $\\pi$, and from distribution $\\phi_2 \\sim \\mathcal{N}(\\mu_2, \\sigma_2^2)$ with probability $(1-\\pi)$. $\\pi$ thus determines our class proportions - $\\pi$ is the proportion of data falling into the first class.\n",
    "\n",
    "Given this definitional background, let's look into how we estimate these parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### E-Step\n",
    "\n",
    "Recall that, in the E-step, we estimate soft class memberships given our estimates of parameters. Specifically, we can calculate the posterioir probability $Z_i$ - the probability that point $i$ falls into cluster $1$ - for each data point:\n",
    "\n",
    "$$\\hat{\\mathit{Z}_i} = \\frac{\\hat{\\pi}\\hat{\\phi}_1(X_i)}{\\hat{\\pi}\\hat{\\phi}_1(X_i) + (1 - \\hat{\\pi})\\hat{\\phi}_2(X_i)}$$\n",
    "\n",
    "Where $\\phi_n(X_i)$ represents the value of the pdf of the gaussian distribution $\\phi_n$ at point $X_i$. For $\\phi_n$, this is\n",
    "\n",
    "$$\\frac{1}{\\sqrt{2\\sigma_n^2\\pi}} e^{\\left(-\\frac{(x - \\mu_n)^2}{2 \\sigma_n^2}\\right)}$$\n",
    "\n",
    "From this we can get \"soft\" class memberships, in the range $(0, 1)$, for each data point.\n",
    "\n",
    "We have provided an implementation of the E-step in python below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def normal_pdf(x, mu, sigma_sq):\n",
    "    return 1/np.sqrt(2*sigma_sq*np.pi) * np.e**(-(x - mu)**2/(2*sigma_sq))\n",
    "\n",
    "def est_Zs(x_i, pi, mu_1, mu_2, sigma_1, sigma_2):\n",
    "    pdf_1 = normal_pdf(x_i, mu_1, sigma_1)\n",
    "    pdf_2 = normal_pdf(x_i, mu_2, sigma_2)\n",
    "    return pi*pdf_1/(pi*pdf_1 + (1-pi)*pdf_2)\n",
    "\n",
    "# Start with a simple guess for the class parameters\n",
    "Z_simple = est_Zs(X_simple, 0.5, 60, 100, 10, 10)\n",
    "Z_simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M-Step\n",
    "\n",
    "Now, using these soft class membership scores, we can update our parameters $\\hat{\\pi}$, $\\hat{\\mu}_1$, $\\hat{\\mu}_2$, $\\hat{\\sigma}_1^2$, $\\hat{\\sigma}_2^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def est_pi_hat(Z):\n",
    "    return np.mean(Z)\n",
    "\n",
    "def est_u_hat(i, X, Z):\n",
    "    if i == 0:\n",
    "        return (np.sum(Z * X)) / np.sum(Z)\n",
    "    else:\n",
    "        return (np.sum((1 - Z) * X)) / np.sum(1 - Z)\n",
    "\n",
    "def est_sigma_hat(i, X, Z, mu_hat):\n",
    "    if i == 0:\n",
    "        return (np.sum(Z * (X - mu_hat)**2)) / np.sum(Z)\n",
    "    else:\n",
    "        return (np.sum((1 - Z) * (X - mu_hat)**2)) / np.sum(1 - Z)\n",
    "    \n",
    "pi_hat = est_pi_hat(Z_simple)\n",
    "u_hat_1 = est_u_hat(0, X_simple, Z_simple)\n",
    "u_hat_2 = est_u_hat(1, X_simple, Z_simple)\n",
    "sigma_hat_1 = est_sigma_hat(0, X_simple, Z_simple, u_hat_1)\n",
    "sigma_hat_2 = est_sigma_hat(1, X_simple, Z_simple, u_hat_2)\n",
    "\n",
    "print(\"Initial values were 0.5, 60, 100, 10, 10\")\n",
    "print(\"Values after one iteration:\")\n",
    "print(pi_hat, u_hat_1, u_hat_2, sigma_hat_1, sigma_hat_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Repeat until satisfied\n",
    "\n",
    "As with k-means, when should we be satisfied? And does our gratification come in a single lifetime?\n",
    "\n",
    "Based on the two update steps above, what are stopping criteria would you suggest?\n",
    "\n",
    "In the next cell, we implement EM with a criterion to stop when the norm $\\left|Z_{old}-Z_{new}\\right|$ falls below a certain threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initial guess\n",
    "pi_hat, u_hat_1, u_hat_2, sigma_hat_1, sigma_hat_2 = 0.5, 80, 90, 10, 10\n",
    "Z_simple = est_Zs(X_simple, pi_hat, u_hat_1, u_hat_2, sigma_hat_1, sigma_hat_2)\n",
    "pi_hat = est_pi_hat(Z_simple)\n",
    "u_hat_1 = est_u_hat(0, X_simple, Z_simple)\n",
    "u_hat_2 = est_u_hat(1, X_simple, Z_simple)\n",
    "sigma_hat_1 = est_sigma_hat(0, X_simple, Z_simple, u_hat_1)\n",
    "sigma_hat_2 = est_sigma_hat(1, X_simple, Z_simple, u_hat_2)\n",
    "old_Z = Z_simple\n",
    "Z_simple = est_Zs(X_simple, pi_hat, u_hat_1, u_hat_2, sigma_hat_1, sigma_hat_2)\n",
    "\n",
    "count = 1\n",
    "while np.linalg.norm(old_Z - Z_simple) > 1e-3:\n",
    "    count += 1\n",
    "    pi_hat = est_pi_hat(Z_simple)\n",
    "    u_hat_1 = est_u_hat(0, X_simple, Z_simple)\n",
    "    u_hat_2 = est_u_hat(1, X_simple, Z_simple)\n",
    "    sigma_hat_1 = est_sigma_hat(0, X_simple, Z_simple, u_hat_1)\n",
    "    sigma_hat_2 = est_sigma_hat(1, X_simple, Z_simple, u_hat_2)\n",
    "    old_Z = Z_simple\n",
    "    Z_simple = est_Zs(X_simple, pi_hat, u_hat_1, u_hat_2, sigma_hat_1, sigma_hat_2)\n",
    "\n",
    "print(\"{} steps to converge\".format(count))\n",
    "print(\"Calculated parameters:\")\n",
    "print(pi_hat, u_hat_1, u_hat_2, sigma_hat_1, sigma_hat_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How well do these distributions model the true values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Close enough for my standards\n",
    "\n",
    "ax = pd.DataFrame({\"Data 8 Score\": X_simple}).plot(kind=\"density\")\n",
    "pd.DataFrame({\"Scores for Class 0 Students\": X_simple[Z_simple > 0.5]}).plot(kind=\"density\", ax = ax)\n",
    "pd.DataFrame({\"Scores for Class 1 Students\": X_simple[Z_simple < 0.5]}).plot(kind=\"density\", ax = ax)\n",
    "plt.axvline(84.3, color=\"black\")\n",
    "plt.title(\"Student scores in Data 8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad! We've clearly found our two groups. Now, let's try doing this on our full, 2D, 3-class data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Part 2 - multiple dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "From here on out, we'll use sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Using sklearn's GaussianMixture estimator, we can estimate parameters for all classes, in multiple dimensions. Now, for each class, $\\mu_i$ is a length-2 vector, and $\\sigma^2_i$ is a 2x2 covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "colors = [\"green\", \"blue\", \"red\"]\n",
    "\n",
    "def make_ellipses(gmm, ax):\n",
    "    for n, color in enumerate(colors):\n",
    "        covariances = gmm.covariances_[n][:2, :2]\n",
    "        v, w = np.linalg.eigh(covariances)\n",
    "        u = w[0] / np.linalg.norm(w[0])\n",
    "        angle = np.arctan2(u[1], u[0])\n",
    "        angle = 180 * angle / np.pi  # convert to degrees\n",
    "        v = 2. * np.sqrt(2.) * np.sqrt(v)\n",
    "        ell = mpl.patches.Ellipse(gmm.means_[n, :2], v[0], v[1],\n",
    "                                  180 + angle, color=color)\n",
    "        ell.set_clip_box(ax.bbox)\n",
    "        ell.set_alpha(0.5)\n",
    "        ax.add_artist(ell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Fitting our model, we can see that it predicts our simple data quite well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model = GaussianMixture(3)\n",
    "model.fit(X)\n",
    "preds = model.predict(X)\n",
    "ax = plt.subplot(111)\n",
    "make_ellipses(model, ax)\n",
    "plt.scatter(X.T[0], X.T[1], color=[colors[i] for i in preds])\n",
    "plt.title(\"GMM predictions and isoprobability contours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "But, let's consider a more complicated case. Now say we still are interested in finding out which startups students interned at - but instead of merely looking at two courses, we have scores in 10 courses availible to us. Let's look at student scores in the first two classes we have availible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "np.random.seed(1337)\n",
    "vals = make_classification(n_features=10, \n",
    "                           n_informative=10, \n",
    "                           n_redundant=0, \n",
    "                           n_classes=3, \n",
    "                           n_clusters_per_class=1)\n",
    "X = vals[0] * 4 + np.ones(10)*85\n",
    "y = vals[1]\n",
    "plt.scatter(X.T[0], X.T[1])\n",
    "plt.title(\"Scatter plot of Stat 134 grade vs. CS 162 grade\")\n",
    "plt.xlabel(\"Stat 134 grade\")\n",
    "plt.ylabel(\"CS 162 grade\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classes aren't clearly visible, at least, not in two dimensions. Training a GMM on the full 100-dimensional dataset gives the following results, which don't line up too well with the true classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model = GaussianMixture(3)\n",
    "model.fit(X.T[:2].T)\n",
    "preds = model.predict(X.T[:2].T)\n",
    "plt.subplot(121)\n",
    "plt.scatter(X.T[0], X.T[1], color=[colors[i] for i in preds])\n",
    "plt.title(\"GMM Predictions\")\n",
    "plt.subplot(122)\n",
    "plt.scatter(X.T[0], X.T[1], color=[colors[i] for i in y])\n",
    "plt.title(\"True Clusters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is, the underlying data isn't very gaussian. We can remedy this, to an extent, by performing PCA and then training our GMM on the principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pcomps = PCA()\n",
    "pcomps.fit(X)\n",
    "pc_X = pcomps.transform(X)\n",
    "\n",
    "model = GaussianMixture(3)\n",
    "model.fit(pc_X.T[:2].T)\n",
    "preds = model.predict(pc_X.T[:2].T)\n",
    "plt.subplot(121)\n",
    "plt.scatter(pc_X.T[0], pc_X.T[1], color=[colors[i] for i in preds])\n",
    "plt.title(\"GMM Predictions\")\n",
    "plt.subplot(122)\n",
    "plt.scatter(pc_X.T[0], pc_X.T[1], color=[colors[i] for i in y])\n",
    "plt.title(\"True Clusters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better results!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Comparion with k-Means\n",
    "\n",
    "How does EM clustering differ from k-Means?\n",
    "\n",
    "k-Means:\n",
    "* Easier to explain and implement.\n",
    "* Generally simpler to compute.\n",
    "* Hard assignment of points to a cluster.\n",
    "* Dependent on cluster center.\n",
    "\n",
    "EM clustering:\n",
    "* Soft assignment of points to a cluster.\n",
    "* Dependent the probability of beloninging to a specific cluster.\n",
    "* Biased towards spherical (or ellipsoid) groups.\n",
    "* Better performance with groups that vary in size."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

